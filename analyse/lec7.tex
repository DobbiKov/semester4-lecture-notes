\section{Extrema et points critiques}
\begin{definition}
    Extremum (local) de $f$ est un minimum ou un maximum (local) de  $f$
    \begin{itemize}
        \item $X_0$ est un maximum local de $f$ si: $\exists \delta > 0$ tel que
            \[
            \forall X \in D, f(X) \le f(X_0) \text{ avec } d(X, X_0) \le \delta
            \] 
        \item $X_0$ est un minimum local de $f$ si: $\exists \delta > 0$ tel que
            \[
            \forall X \in D, f(X) \ge f(X_0) \text{ avec } d(X, X_0) \le \delta
            \] 
    \end{itemize}
\end{definition}

\begin{definition}
    Soit $f: D \to \R$ et $X_0 \in D$, alors si
     \[
         \vec{\nabla}f(X_0) = \vec{0}
    \] 
    donc $X_0$ est un \textbf{point critique}.
\end{definition}
\begin{intuition}
   Le lien entre les extremums et le point critique: 
   \begin{enumerate}
       \item pour que l'extremum existe, il faut qu'il existe au moins un point critique - c'est un critère nécessaire \underline{mais pas suffisant}.
       \item tout extremum local est un point critique
   \end{enumerate}
   Les points critiques falicites la recherche des extremums locaux.
\end{intuition}

\begin{theorem}
    Soit $f: D \longrightarrow \R$ différentiable, $D$ ouvert et  $X_0 \in D$ (sinon, si $D$ pas ouvert, il faut  $X_0 \in \operatorname{Int}(D)$) alors:
    \[
        X_0 \text{ extremum local } \implies X_0 \text{ point critique} 
    \] 
\end{theorem}

\begin{eg} Pas tout point critique est un extremum local
\begin{figure}[H]
    \centering
    \incfig{point-critique-nestpas-extremum-local}
    \caption{Point critique qui n'est pas un extremum local}
    \label{fig:point-critique-nestpas-extremum-local}
\end{figure}
\end{eg}

\section{Dérivées partielles d'ordre $\ge 2$}
\begin{definition}
    Soit $D$, alors  $f: D \to \R$ est $\mathcal{C}^k$ si  $f: D \to \R$ est $\mathcal{C}^1$ et  $\partial_{x_i}f: D \to \R$ sont $C^{k-1}$
\end{definition}

\begin{definition}
    Soient $\alpha = (\alpha_1, \ldots, \alpha_n)$ \quad $\alpha_i \in \N$. On pose
    \[
        \partial_{x}^{\alpha}f = \frac{\partial^{\alpha_1}}{\partial x_1^{\alpha_1}} \cdot \ldots \cdot \frac{\partial^{\alpha_n}}{\partial x_n^{\alpha_n}}
    \] 
    est la notation pour la dérivée d'ordre supérieure.
\end{definition}
\[
    \frac{\partial}{\partial x_1}\frac{\partial}{\partial x_2}\frac{\partial}{\partial x_1} f \overset{?}{=}  \frac{\partial^2}{\partial x_1^2}\frac{\partial}{\partial x_2}f
\] 

\begin{theorem}\label{thm:lemme-de-schwarz}
    Lemme de Schwarz
    \par
    Si $f \in \mathcal{C}^2(D)$ alors 
    \[
        \displaystyle \frac{\partial^2 f}{\partial x_i\partial x_j}(X) = \frac{\partial^2 f}{\partial x_j\partial x_i}(X) \qquad \forall X \in D, \forall i, j
    \] 
\end{theorem}
\begin{eg} où une fonction admet des dérivées partielles d'ordre supérieure mais $\displaystyle \frac{\partial^2 f}{\partial x_i\partial x_j}(X) \neq  \frac{\partial^2 f}{\partial x_j\partial x_i}(X)$
   \[
   f(x_1, x_2) = \begin{cases}
       x_1x_2 \frac{x_1^2 - x_2^2}{x_1^2 + x_2^2} \text{ si } (x_1, x_2) \neq (0, 0)\\
       0 \text{ si } (x_1, x_2) = 0
   \end{cases}
   \]  
   \[
   r^2 \sin(\theta)\cos(\theta)\cos(2\theta) = \frac{1}{4}r^2\sin(4\theta)
   \] 
   On calcule $\displaystyle \frac{\partial^2f}{\partial_{x_1}\partial_{x_2}}(0, 0)$?
       C'est $\displaystyle \frac{\partial}{\partial x_1}g(x_1)$ en $x_1 = 0$ pour $\displaystyle g(x_1) = \frac{\partial f}{\partial x_2}(x_1, x_2)|_{x_2 = 0}$. Calcul de $g(x_1)$:
    \begin{enumerate}
        \item si $x_1 \neq 0$ $\frac{\partial f}{\partial x_2}(x_1, x_2) = x_1 \frac{x_1 ^ 2 - x_2^2}{x_1^2 + x_2^2}$, donc si $x_1 \neq 0$ $\frac{\partial f}{\partial x_2}(x_1, 0) = x_1$ 
        \item si $x_1 = 0$ $f(0, x_2) = 0$
    \end{enumerate}
    Conclusion:
    \[
    \frac{\partial f}{\partial x_2}(x_1, 0) = x_1 \quad \forall x_1
    \] 
    donc: 
    \[
    \frac{\partial}{\partial x_1}\frac{\partial}{\partial x_2}f(0, 0) = 1
    \] 
    $\displaystyle \frac{\partial}{\partial x_2}\frac{\partial}{\partial x_1}f(0, 0) = ?$. On voit que, $f(x_2, x_1) = -f(x_1, x_2)$ donc 
    \[
    \frac{\partial}{\partial x_2}\frac{\partial}{\partial x_1}f(0, 0) = - \frac{\partial}{\partial x_1}\frac{\partial}{\partial x_2}f(0, 0) = -1
    \] 
\end{eg}

\section{Formule de Taylor à l'ordre 2}
\begin{definition}
    Soit $f \in \mathcal{C}^2(D)$. Matrice hessienne: matrice  $n \times n$
    \[
    H_f(X_0) = \begin{bmatrix} \frac{\partial^2}{\partial x_i \partial x_j}(X_0) \end{bmatrix} 1\le i,j \le n
    \] 
    Le lemme \ref{thm:lemme-de-schwarz} nous donne que $H_f(X_0)$ est symmetrique si $f \in \mathcal{C}^2(D)$
\end{definition}
Rappel: 
\[
    \vec{\nabla}f(X_0) = \begin{pmatrix} \frac{\partial f}{\partial x_1}(X_0) \\ \vdots \\ \frac{\partial f}{\partial x_n}(X_0) \end{pmatrix} 
\] 

\begin{theorem} De Taylor à l'ordre 2 \par
    Soit $f \in \mathcal{C}^2(D)$,  $X_0 \in D$. Alors  
    \begin{align*}
        f(X_0 + \vec{X}) = f(X_0) + \vec{\nabla }f(X_0) \cdot \vec{X} + \frac{1}{2}\vec{X} \cdot H_f(X_0)\vec{X}
    \end{align*}
    exemple en $\R^1$
    \[
    f(x_0 + x) = f(x_0) + f'(x_0)x + \frac{1}{2}f''(x_0)x^2 + \ldots
    \] 
\end{theorem}
\begin{intuition}
   Alors, la matrice hessienne sert à calculer la dérivée d'ordre 2. 
\end{intuition}

\section{Un rappel d'algèbre linéaire et le lien avec l'analyse}
\[
    \vec{X} \cdot A\vec{X} = \sum_{1\le i,j \le n}^{} x_ia_{i,j}x_j
\] 
Si $\vec{X} = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ $A = \begin{bmatrix} a_{i,j} \end{bmatrix}$ on a: $X \mapsto X \cdot AX$ à étudier. Si $A = A^T, A \in \mathcal{M}_n(\R)$
\begin{center}
   "$A$ admet une base orthonormée de vecteurs propres" 
\end{center}
Il existe une base $\vec{u_1}, \ldots, \vec{u_n}$ de $\R^n$ avec $\vec{u_i} \cdot \vec{u_j} = \delta_{i, j}$ ($1$ si  $i = j$ et  $0$ sinon) et des réels $\lambda_1, \ldots, \lambda_n$($\lambda_i = \lambda_j$ possible) tels que
\[
    A\vec{u_i} = \lambda_i\vec{u_i}
\] 
\[
    \vec{X} = \sum_{j=1}^{n} y_j\vec{u_j}
\] 
\[
    \vec{X} \cdot \vec{u_i} = \sum_{j=1}^{n} y_j\vec{u_j}\vec{u_i} = y_i
\] 
\begin{align*}
    \|\vec{X}\|^2 = \vec{X} \cdot \vec{X} &= \left( \sum_{j=1}^{n} y_j\vec{u_j} \right) \cdot \left( \sum_{i=1}^{n} y_i\vec{u_i} \right) \\
                                          &= \sum_{j=1}^{n} \sum_{i=1}^{n} y_jy_i\vec{u_j}\cdot \vec{u_i}\\ 
                                          &= \sum_{j=1}^{n} y_j^2
\end{align*}

\begin{align*}
    A\vec{X} = A \sum_{j=1}^{n} y_j\vec{u_j} = \sum_{j=1}^{n} y_jA\vec{u_j} = \sum_{j=1}^{n} \lambda_jy_j\vec{u_j}
\end{align*}
\[
    \vec{X} \cdot A\vec{X} = \sum_{i=1}^{n} \lambda_iy_i^2
\] 
\begin{enumerate}
    \item si $\lambda_i > 0$ ($1 \le i \le n$)
        \[
        C = \min \lambda_i > 0
        \] 
        \[
        X \cdot AX \ge C \sum_{i=1}^{n} y_i^2 = C \|X\|^2
        \] 
    \item si $\lambda_i < 0$ ($1 \le i \le n$)
        \[
        -C = \max \lambda_i < 0
        \] 
        \[
        X \cdot AX \le -C\|X\|^2
        \] 
\end{enumerate}
\begin{eg}
   $n = 2$ 
   \[
   f(y_1, y_2) = -y_1^2 + 3y_2^2
   \] 
   \[
   \lambda_1 = -1 \qquad \lambda_2 = 3
   \] 
   \[
   f(y_1, 0) < f(0, 0) < f(0, y_2)
   \] 
\end{eg}

\section{Nature des points critiques}
\begin{theorem}\label{thm:nature-des-points-critiques}
    (Nature des points critiques) \par
    Soient $f \in \mathcal{C}^2(D)$,  $X_0 \in D$, $D$ ouvert et  $\vec{\nabla }f(X_0) = \vec{0}$
    \begin{enumerate}
        \item si toutes les valeurs propres de $H_f(X_0)$ sont $> 0$ (resp $< 0$) $X_0$ est minimum (resp. maximum) local.
        \item si toutes les valeurs propres de $H_f(X_0)$ sont \underline{non nulles} mais pas de même signe, $X_0$ n'est pas un extremum local: $X_0$ est un point selle (un col).
        \item si 0 valeurs propres de $H_f(X_0)$, pas de conclusion, ($X_0$ point critique dégénéré) i.e on ne peut rien conclure
    \end{enumerate}
\end{theorem}

\begin{preuve} du théorème ~\ref{thm:nature-des-points-critiques}
   \[
       f(X_0 + X) - f(X_0) = \frac{1}{2}X\cdot H_f(X_0)X + \|X\|^2\epsilon(X)
   \]  
   .
   \begin{enumerate}
       \item si $\lambda_i > 0$  $\frac{1}{2}X \cdot H_f(X_0)X \ge C\|X\|^2$ $C > 0$
            \[
           f(X_0 + X) - f(X_0) \ge \|X\|^2(C + \epsilon(X)) \ge \frac{C}{2}\|X\|^2 \text{ si } \|X\| \text{ assez petit }
           \] 
           \[
           \implies X_0 \text{ minimum local}
           \] 
        \item si $\lambda_1 < 0$ et  $\lambda_2 > 0$
             \[
                 H_f(X_0)\vec{u_i} = \lambda_i\vec{u_i}
            \] 
            \[
                f(X_0 + t\vec{u_i}) = f(X_0) + \frac{1}{2} \lambda_it^2 + t^2\epsilon(t)
            \] 
            \[
                \epsilon(t\vec{u_i}) = \epsilon(t)
            \] 
            \[
                f(X_0 + t\vec{u_i}) - f(X_0) = t^2 (\frac{1}{2}\lambda_i + \epsilon(t))
            \] 
            si $i = 1$  $< 0$  $|t|$ petit,  $i = 2$  $> 0$  $|t|$ petit, alors  $X_0$ n'est pas un extremum local
   \end{enumerate}
\end{preuve}

\begin{eg}
   \[
   f(x, y) = \frac{1}{2}(x^2 - y^2)
   \]  
   \[
       H_f(0, 0) = \begin{pmatrix} 1 & 0\\ 0 & -1 \end{pmatrix} 
   \] 
   \[
       I_f = \{(x, y, z): z = \frac{1}{2}(x^2 - y^2)\}
   \] 
\begin{figure}[H]
    \centering
    \incfig{exemple-point-selle}
    \caption{Exemple de point selle.}
    \label{fig:exemple-point-selle}
\end{figure}
Les lignes rouges représentent les dérivées partielles et on voit bien que les uns sont croissants et les autres décroissants, donc ce point n'est ni le minimum ni le maximum
\end{eg}

\begin{eg}
$n = 2$
 \[
A = \begin{pmatrix} 
    a_{1,1} & a_{1, 2}\\
    a_{2, 1} & a_{2, 2}
\end{pmatrix} 
\] 
$(a_{1,2} = a_{2, 1})$
\par
Valeurs propres: racines du pol. Caractéristique:  
\[
    P(\lambda) = \det(A - \lambda I) = \begin{vmatrix} a_{1,1} - \lambda & a_{1, 2} \\ a_{2, 1} & a_{2,2} - \lambda \end{vmatrix} = (\lambda - a_{1,1})(\lambda - a_{2,2}) -  a_{1,2}a_{2,1}
\] 
\[
    \lambda^2 - (a_{1,1} + a_{2,2})\lambda + a_{1,1}a_{2,2} - a_{2,1}a_{1,2}
\] 
\[
    a_{1,1} + a_{2,2} = Tr(A)
\] 
\[
    a_{1,1}a_{2,2} - a_{2,1}a_{1,2} = \det(A)
\] 
\[
    x^2 - Sx + P = x^2 - (\lambda_1 + \lambda_2)x + \lambda_1\lambda_2
\] 
\begin{align*}
    &\det(A) = \text{ produit des valeurs propres}\\
    &Tr(A) = \text{ somme des valeurs propres}
\end{align*}
\[
A = H_f(X_0)
\] 
\begin{enumerate}
    \item si $\det(A) < 0$,  $X_0$ point col
    \item si $\det(A) > 0$
         \begin{enumerate}
            \item $Tr(A) > 0$, $X_0$ minimum
            \item  $Tr(A) < 0$, $X_0$ maximum
        \end{enumerate}
    \item $\det(A) = 0$,  $X_0$ point critique dégénéré
\end{enumerate}
\end{eg}
