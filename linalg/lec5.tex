\chapter{Réduction des endomorphismes}
En écrivant ce chapitre, j'étais inspiré par les videos du chaîne \textit{3blue1brown} que je vous conseille de regarder, au moins le playlist concernant l'algèbre linéaire. La deuxieme source de l'inspiration était le livre de Joseph Grifone \cite{grifone}.
\section{Introduction}
Dans le chapitre précédent on a étudié une notion d'une base orthonormale dont les utilités sont: simplification des calculs des coordonnées dans une base et calcule d'une projection. 
Cette notion est l'un des premiers pas vers l'étude de SVD\footnote{Singular Value Decomposition} qui est appliqué dans plusieurs domaines, e.g: la réduction des tailles d'images.
\par
Dans ce chapitre on continue l'étude des bases pour pouvoir finalement comprendre le SVD. On va étudier la réduction des endomorphismes, \textit{to be more precise} la diagonalisation et la triagonalisation. Pour commencer: un petit exo:
\begin{ex}
   Calculer 
   \[
   \begin{bmatrix} 
       3 & 1\\
       0 & 2
   \end{bmatrix}^{15} = \underbrace{
       \begin{bmatrix} 
       3 & 1\\
       0 & 2
       \end{bmatrix}
       \cdot
       \ldots
       \cdot
       \begin{bmatrix} 
       3 & 1\\
       0 & 2
       \end{bmatrix}
   }_{15 \text{ fois}}
   \] 
\end{ex}
Cela ne semble pas très facile, n'est-ce pas? Au bout de ce chapitre, on va trouver une façon à simplifier le calcule et à la fin on résoudra cet exercice.
\par
On sait d'algèbre linéaire qu'on peut représenter une matrice d'une application dans des bases différentes, i.e soit une base $\{e_i\}$ de $E$ et  $f$ une application. Alors cette aplication dans la base  $\{e_i\}$ est représentée:
 \[
A = M(f)_{e_i} = \|f(e_1), \ldots, f(e_n)\|
\] 
Soit $\{e_i'\}$ une autre base de  $E$, alors on peut représenter l'application  $f$ dans cette base aussi, notons:  $P = P_{e_i \to e_i'}$ une matrice de passage de la base $\{e_i\}$ vers la base  $\{e_i'\}$
 \[
     A' = M(f)_{e_i'} = P^{-1}AP = \|f(e_1'), \ldots, f(e_n')\|_{e_i'}
\] 
\begin{definition}\label{def:matrice-diagonalisable}
    La matrice $A$ est \textbf{diagonalisable} s'il existe une matrice semblable\footnote{$A$ est semblable à  $A'$ s'il existe une matrice de passage  $P$ telle que  $A' = P^{-1}AP$}  $A'$ diagonale:
     \[
         A' = 
         \begin{bmatrix} 
         a_{1,1} & 0 & \ldots & 0 \\
         0 & a_{2,2} & \ddots & \vdots\\
         \vdots & \ddots & \ddots & 0\\
         0 & \ldots & 0 & a_{n,n}
        \end{bmatrix} 
    \] 
\end{definition}
\begin{definition}
    La matrice $A$ est \textbf{triagonalisable} s'il existe une matrice semblable  $A'$ triangulaire (supérieure/inférieure) 
     \[
         A' = 
         \begin{bmatrix} 
         a_{1,1} & a_{1,2} & \ldots & a_{1,n} \\
         0 & a_{2,2} & \ddots & \vdots\\
         \vdots & \ddots & \ddots & a_{n-1,n}\\
         0 & \ldots & 0 & a_{n,n}
        \end{bmatrix} \text{ ou } 
         A' = 
         \begin{bmatrix} 
         a_{1,1} & 0 & \ldots & 0 \\
         a_{2, 1} & a_{2,2} & \ddots & \vdots\\
         \vdots & \ddots & \ddots & 0\\
         a_{n, 1} & \ldots & a_{n, n-1} & a_{n,n}
        \end{bmatrix} 
    \] 
\end{definition}
Alors les problèmes de ce chapitre qu'on va résoudre sont:
\begin{enumerate}
    \item Détérminer si un endomorphisme $f$ est diagonalisable/triagonalisable i.e s'il existe telle matrice  $A'$.
    \item Détérminer la matrice de passage $P$ et la matrice $A'$.
\end{enumerate}
Dans tout le chapitre on suppose que l'espace vectoriel $E$ est de dimension finie.
\section{Vecteurs propres - Eigenvectors}
Commençons par clarification d'une notion de l'application linéaire et sa matrice. Preonons pour ça la matrice de l'exercice du début du chapitre:
\[
A = \begin{bmatrix} 
       3 & 1\\
       0 & 2
   \end{bmatrix}
\] 
Cetter matrice transforme l'espace vectoriel qu'on le donne, ou en simplifiant, elle transforme chaque vecteur de l'espace vectoriel. Prennons un vecteur $v_3 = \begin{pmatrix} 1 \\ 1 \end{pmatrix} $, en appliquant $A$ on obtient:
 \[
Av_3 = \begin{bmatrix} 
       3 & 1\\
       0 & 2
   \end{bmatrix}\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 3 \\ 0 \end{bmatrix} + \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{bmatrix} 4 \\ 2 \end{bmatrix} 
\] 

\begin{center}
    
\begin{tikzpicture}
    \begin{axis}[
        axis lines=middle, 
        grid=major, 
        xlabel={$x$}, 
        ylabel={$y$}, 
        ymin=-1, ymax=3,
        xmin=-1, xmax=5
    ]
        % Define original vectors
        \addplot[->, thick, blue] coordinates {(0,0) (1,1)};
        \node[right] at (axis cs:1,1) {$v_3$};

        % Define transformed vectors
        \addplot[->, thick, blue, dashed] coordinates {(0,0) (4,2)};
        \node[right] at (axis cs:4,2) {$A v_3$};
    \end{axis}
\end{tikzpicture}
\end{center}
On remarque que le vecteur $Av_3$ n'est plus situé en même ligne que le vecteur $v_3$, ce qui est logique car si les vecteurs étaient en même lignes après une transformation, cela n'aurait pas de sens.
Par contre, parfois il y'a des cas, quand le vecteur appliqué à la matrice reste en même ligne, par exemple le vecteur $v_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix} $, avec $Av_2 = \begin{pmatrix} -2 \\ 2 \end{pmatrix} = 2v_2 $

\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        axis lines=middle, 
        grid=major, 
        xlabel={$x$}, 
        ylabel={$y$}, 
        ymin=-1, ymax=5,
        xmin=-5, xmax=5
    ]
        % Define original vectors
        \addplot[->, thick, red] coordinates {(0,0) (-1, 1)};
        \node[above] at (axis cs:-1,1) {$v_2$};
        
        \addplot[->, thick, blue] coordinates {(0,0) (1,1)};
        \node[right] at (axis cs:1,1) {$v_3$};

        % Define transformed vectors
        \addplot[->, thick, red, dashed] coordinates {(0,0) (-2,2)};
        \node[above] at (axis cs:-2,2) {$A v_2$};
        
        \addplot[->, thick, blue, dashed] coordinates {(0,0) (4,2)};
        \node[right] at (axis cs:4,2) {$A v_3$};

        \addplot[->, thick, orange, dotted] coordinates {(0,0) (-4,4)};
        \node[above] at (axis cs:-4,4) {$A A v_2$};
    \end{axis}
\end{tikzpicture}
\end{center}
Et c'est pas uniquement le cas du vecteur $\begin{pmatrix} -1 \\ 1 \end{pmatrix} $, en prennant n'importe quel vecteur engendré pas $v = \begin{pmatrix} -1 \\ 1 \end{pmatrix} $, on obtiendra $Av = 2v$.
Tels vecteurs $v$ et les scalaire (ici: 2) sont appelés vecteurs propres et valeurs propres respectivement. Alors, on a la définition formelle:
\begin{definition}
    Soit $f$ un endomorphisme dans  $E$ et un vecteur  $v \in E$ est dit \textbf{vecteur propre} de $f$ si:
     \begin{enumerate}
        \item $v \neq 0$
        \item Il existe un réél $\lambda$ tel que  $f(v) = \lambda v$
    \end{enumerate}
    Le scalaire $\lambda \in \R$ est dit \textbf{valeur propre} correspondante à $v$.
\end{definition}
\begin{intuition}
   Les vecteurs propres sont les vecteurs qui sous l'action de $f$ ne changent pas de diréctions, justement la longueure (même pas toujours). Cela simplifie le calcul de tel vecteurs. Pouvez-vous calculer  $A^3v_3$? Pas très facile, alors le vecteur  $A^3v_2$? 
   \[
    Av_2 = 2v_2 \implies A^2v_2 = 2\cdot 2v_2 = 4v_2 \implies A^3v_2 = 2 \cdot 4v_2 = 8v_2 = \begin{pmatrix} -8 \\ 8 \end{pmatrix} 
   \] 
   C'est cool, n'est-ce pas?
\end{intuition}
Par contre, ce n'est pas la seule utilité des vecteurs propres et on va revenir ici pour en discuter, mais d'abord, comment trouver tels vecteurs?

\section{Recherche des valeurs propres}
On cherche des vecteurs qui sous l'action de l'endomorphisme $f$ sont mis à l'échelle par un facteur de  $\lambda \in \R$, alors on est sensé de résoudre cette équation:
\begin{align*}
    && f(v) &= \lambda v\\
    \iff&& Av &= \lambda v \quad \text{ en notation matricielle}\\
    \iff&& Av &= \lambda (Iv) \quad \text{ où } I \text{ est une matrice identité}\\
    \iff&& Av - \lambda Iv &= 0\\
    \iff&& (A - \lambda I)v &= 0
\end{align*}
Donc, on doit étudier l'application $(A - \lambda I)$ et la connecter à la notion des détérminants. Rappelle: si le détérminant d'une matrice n'est pas nul, cette matrice (i.e endomorphisme) est injective. Dans notre cas, si  $\det(A - \lambda I)$ était nul, le seul vecteur  $v$ qui donnerait  $(A - \lambda I)v = 0$ était le vecteur nul  $v = 0$ car  $(A - \lambda I)$ est linéaire et (comme on a suppoé) injective. 
\par
Par contre, d'après la définition, les vecteurs propres ne sont pas nul, alors le cas injectif ne convient pas, donc pour avoir des vecteurs propres l'application $(A - \lambda I)$ doit ne pas être injectif ce qui équivaut à dire que  $\det(A - \lambda I) = 0$. Alors, on est sensé de calculer le détérminant suivant:
\[
\det(A - \lambda I) = \det \left(\begin{bmatrix} 
    a_{1,1} & a_{1, 2} & \ldots & a_{1, n}\\
    a_{2,1} & a_{2, 2} & \ldots & a_{2, n}\\
    \ldots & \ldots & \ldots & \ldots\\
    a_{n,1} & a_{n,2} & \ldots & a_{n,n}
    \end{bmatrix}  - 
    \begin{bmatrix} 
        \lambda & 0 & \ldots & 0 \\
        0 & \lambda & \ldots & 0 \\
        \ldots & \ldots & \ldots & \ldots\\
        0 & 0 & \ldots & \lambda
    \end{bmatrix}\right) = 
    \begin{vmatrix} 
    a_{1,1} - \lambda & a_{1, 2} & \ldots & a_{1, n}\\
    a_{2,1} & a_{2, 2} - \lambda & \ldots & a_{2, n}\\
    \ldots & \ldots & \ldots & \ldots\\
    a_{n,1} & a_{n,2} & \ldots & a_{n,n} - \lambda
    \end{vmatrix}
\] 
En développant ce détérminant on obtient une équiation du type:
\[
    (-1)^n\lambda^n + a_{n-1}\lambda^{n-1} + \ldots + a_1\lambda + a_0 = 0
\] 
dont les racines sont les valeurs propres de $f$ (rappelle: valeur propre est un facteur $\lambda$).
Ne vous concentrez pas trop sur cette équation pour l'instant, on va y revenir.
\begin{prop}\label{prop:polynome-caracteristique}
   Soit $f$ un endomorphisme dans un espace vectoriel  $E$ de dimension finie  $n$ et $A$ la matrice représentative de  $f$ dans une base de  $E$. Les valeurs propres de  $f$ sont les racine du polynôme:
   \[
   P_f(\lambda) = \det(A - \lambda I)
   \] 
   Ce polynôme est dit \textbf{polynôme caracteristique} de $f$.
\end{prop}
\begin{definition}
    L'ensemble des valeurs propres de $f$ est dit  \textbf{spectre} de $f$ et est noté  $\operatorname{Sp}_K(f)$ ou  $\operatorname{Sp}_K(A)$ si $A$ une matrice de  $f$.
\end{definition}
Pour clarifier:
\begin{eg}
   Soit $f$ un endomorphisme dans  $\R^2$ dont la matrice représentative dans la base canonique est:
   \[
       \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix} 
   \] 
   Calculons ses valeurs propres:
   \begin{align*}
       && \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix} v &= \lambda v \\
       \iff && \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix} v - \lambda I v &= 0\\
       \iff && \left(\begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix}  - \lambda I\right)v  &= 0\\
       \implies && \det \left( \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix}  - \lambda I \right) &= 0\\
       \implies && \det \left( \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix}  - \lambda \begin{bmatrix} \lambda & 0\\ 0 & \lambda \end{bmatrix}  \right) &= 0\\
       \implies && \det \left( \begin{bmatrix} 3 - \lambda & 1\\ 0 & 2 - \lambda \end{bmatrix}\right) &= 0\\
                && &= (3-\lambda)(2 - \lambda) = 0
   \end{align*}
   On voit bien, que les solutions sont: $\lambda_1 = 3$ et  $\lambda_2 = 2$
\end{eg}
On peut trouver des valeurs propres, néanmoins, on cherchait les \underline{vecteurs} propres. Et on est là:
\section{Recherche des vecteurs propres}
Supposons pour $q \in \N^{*}$ on a déjà trouvé $q$ valeurs propres d'une matrice $\{ \lambda_1, \ldots, \lambda_q \}$, pour trouver les vecteurs propres, il nous reste à trouver la base de:
\[
    \ker(A - \lambda_iI) \quad \forall i \in \{1, \ldots, q\}
\] 
ce qui équivaut à:
\[
\left( A - \lambda_i I \right)v = 0 \quad \forall i \in \{1, \ldots, q\}
\] 

\begin{eg}
   Encore la matrice  
   \[
       A = \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix} 
   \] 
   dans la base canonique de $\R^2$. On a déjà trouvé ses vecteurs propres: $\lambda_1 = 3$ et $\lambda_2 = 2$. Alors, cherchons les vecteurs:
   \[
       \begin{bmatrix} 3 - \lambda_1 & 1 \\ 0 & 2 - \lambda_1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 3 - 3 & 1 \\ 0 & 2 - 3 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ 0 & -1 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = 0 
       \implies \begin{cases}
           y = 0\\
           -y = 0\\
           x \in \R
       \end{cases}
   \] 
   Donc $\ker(A - 3I) = \begin{pmatrix} x \\ 0 \end{pmatrix} =  \operatorname{Vect}(\begin{pmatrix} 1 \\ 0 \end{pmatrix} )$. Voilà, notre premier vecteur propre: $\begin{pmatrix} 1 \\ 0 \end{pmatrix} $. Pour le deuxième:
   \[
       \begin{bmatrix} 3 - \lambda_2 & 1 \\ 0 & 2 - \lambda_2 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 3 - 2 & 1 \\ 0 & 2 - 2 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0 & 0 \end{bmatrix}\begin{bmatrix} x \\ y \end{bmatrix} = 0 
       \implies \begin{cases}
           x + y = 0
       \end{cases} \implies \begin{cases}
           x = -y
       \end{cases}
   \] 
   Donc $\ker(A - 2I) = \begin{pmatrix} -y \\ y \end{pmatrix} = y \begin{pmatrix} -1 \\ 1 \end{pmatrix} = \operatorname{Vect}(\begin{pmatrix} -1 \\ 1 \end{pmatrix} )  $ et voilà le deuxieme vecteur propre: $\begin{pmatrix} -1 \\ 1 \end{pmatrix} $ (c'était notre vecteur $v_2$ au début du chapitre).
\end{eg}

Enfin, la propriété utile:
\begin{prop}
    Soit $A \in \mathcal{M}_n(\R)$ avec ses vecteurs propres: $\{\lambda_1, \ldots, \lambda_n\}$, alors:
    \begin{align*}
        &\operatorname{Tr}(A) = \lambda_1 + \ldots + \lambda_n\\
        &\operatorname{det}(A) = \lambda_1 \cdot  \ldots \cdot  \lambda_n\\
    \end{align*}
\end{prop}

\section{Les endomorphismes diagonalisables}
Revenons sur l'utilité des vecteurs propres. Soit $f$ un endomorphisme de  $E$ dont la base est  $\{e_1, \ldots, e_n\}$ et $\operatorname{Mat}_{e_i}(f) = A$ et la matrice de  $f$ dans cette base. Reprennons l'exemple suivant:
 \begin{eg}
     On a: $A = \begin{bmatrix} 3 & 1\\ 0 & 2 \end{bmatrix} $ dans la base canonique $e_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} $ et $e_2 = \begin{bmatrix} 0 \\ 1 \end{bmatrix} $. On rappelle qu'on a trouvé deux vecteurs propres:
     \[
     \begin{cases}
         v_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \\
         v_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix} 
     \end{cases}
     \] 
     On remarque que ces deux vecteurs sont libres et donc forment une base  de $\R^2$. Essayons de changer la base de $A$ dont on a deux façon:
      \begin{enumerate}
          \item On peut calculer les coordonnées de $f(v_1)$ et $f(v_2)$ dans la base $\{v_1, v_2\}$, on a:
              \begin{align*}
                  &f(v_1) = 3v_1 = 3 \cdot v_1 + 0 \cdot v_2\\
                  &f(v_2) = 2v_2 = 0 \cdot v_1 + 2 \cdot v_2
              \end{align*}
              Et alors $\operatorname{Mat}_{v_i}(f) = \|f(v_1), f(v_2)\|_{v_i} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix} $ 
          \item On peut calculer la matrice $P = P_{e_i \to v_i}$de passage d'une base $\{e_i\}$ vers la base  $\{v_i\}$ et en déduire la matrice de $f$ dans la nouvelle base. On a:
             \[
            \begin{cases}
                v_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} = 1 \cdot e_1 + 0 \cdot e_2 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}_{e_i} \\
                v_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix} = -1 \cdot e_1 + 1 \cdot e_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}_{e_i} \\
            \end{cases}
            \] 
              donc $P = \begin{bmatrix} 1 & -1\\ 0 & 1 \end{bmatrix} $ et $P^{-1} = \begin{bmatrix} 1 & 1 \\ 0 & 1  \end{bmatrix}$ (vous pouvez vérifier le calcul). Et donc:
              \[
                  A' = P^{-1}AP = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \underbrace{\begin{bmatrix} 3 & -2 \\ 0 & 2 \end{bmatrix}}_{AP} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix} 
              \] 
     \end{enumerate}
     Et voilà, la magie, on a trouvé la matrice diagonale.
\end{eg}
Ensuite, généralisons ce qu'on a fait. 
\begin{definition}
    Soit $\lambda \in K$, on note:
    \[
        E_{\lambda} := \{v \in E \mid f(v) = \lambda v \}
    \] 
    $E_{\lambda}$ est un espace vectoriel de $E$ dit  \textbf{espace propre} correspondant à $\lambda$.
\end{definition}
\begin{remark}
   \begin{enumerate}
       \item Si $\lambda$ n'est pas valeur propre de $f$, donc  $E_\lambda = \{0\}$
       \item Si  $\lambda$ est valeur propre, alors:
            \[
                E_\lambda = \{ \text{ vecteurs propres associés à } \lambda \} \cup \{0\} \text{ et } \dim E_\lambda \ge 1
           \] 
   \end{enumerate} 
\end{remark}

\begin{prop}
    Soient $\lambda_1, \ldots, \lambda_p$ des scalaires deux à deux distincts. Alors les espaces propres $E_{\lambda_1}, \ldots, E_{\lambda_p}$ sont en somme directe. Autrement dit, si $\mathcal{B}_1, \ldots, \mathcal{B}_p$ sont des bases de $E_{\lambda_1}, \ldots, E_{\lambda_p}$, la famille $\{\mathcal{B}_1, \ldots, \mathcal{B}_p\}$ est libre (mais pas nécessairement génératrice de $E$).
\end{prop}
\begin{preuve}
Soient $E_{\lambda_1}, \ldots, E_{\lambda_p}$ les espaces propres associés aux valeurs propres $\lambda_1, \ldots, \lambda_p$ d’un endomorphisme $f$ d’un espace vectoriel E. Nous devons montrer que ces sous-espaces sont en somme directe, c’est-à-dire que si un vecteur appartient à leur intersection, alors il est nul.

Prenons un élément v appartenant à leur somme, c’est-à-dire qu’il peut s’écrire sous la forme :
\[
    v = v_1 + v_2 + \cdots + v_p
\] 
avec $v_i \in E_{\lambda_i}$ pour tout $i$.

Puisque chaque $v_i$ est un vecteur propre pour $f$ associé à $\lambda_i$, on a :
\[
    f(v_i) = \lambda_i v_i.
\] 
Appliquons f à la somme :
\[
    f(v) = f(v_1 + v_2 + \cdots + v_p) = f(v_1) + f(v_2) + \cdots + f(v_p).
\] 
En utilisant la linéarité de $f$, cela donne :
\[
    f(v) = \lambda_1 v_1 + \lambda_2 v_2 + \cdots + \lambda_p v_p.
\] 
Or, $v$ est aussi une combinaison de ces mêmes vecteurs :
\[
    v = v_1 + v_2 + \cdots + v_p.
\] 
Donc, en réarrangeant :
\[
    (\lambda_1 v_1 + \lambda_2 v_2 + \cdots + \lambda_p v_p) - (v_1 + v_2 + \cdots + v_p) = 0.
\] 
Ce qui donne :
\[
    (\lambda_1 - 1) v_1 + (\lambda_2 - 1) v_2 + \cdots + (\lambda_p - 1) v_p = 0.
\] 
Factorisons chaque terme :
\[
    (\lambda_1 - \lambda) v_1 + (\lambda_2 - \lambda) v_2 + \cdots + (\lambda_p - \lambda) v_p = 0.
\] 
Or, les $\lambda_i$ sont supposés deux à deux distincts. On en déduit que les coefficients sont différents, et que la somme est nulle uniquement si tous les $v_i$ sont nuls (puisque les espaces propres sont en général en somme directe).

Ainsi, $v = 0$, ce qui prouve que les espaces propres sont en somme directe.
\end{preuve}
Ainsi, les espaces propres sont toujours en sommes directe, mais pas necessairement égale à $E$:
 \[
     E_{\lambda_1} \oplus \ldots \oplus E_{\lambda_p} \underset{\neq}{\subset} E
\] 
ce qu'on a si:
\[
\dim E_{\lambda_1} + \ldots + \dim E_{\lambda_p} < \dim E
\] 

\begin{theorem}
    Soit $f$ un endomorphisme dans  $E$ et  $\lambda_1, \ldots, \lambda_p$ ses valeurs propres, alors les propriétés suivantes sont équivalentes:
    \begin{enumerate}
        \item $f$ est diagonalisable
        \item  $E$ est somme directe de ses espaces propres:  $E = E_{\lambda_1} \oplus \ldots \oplus E_{\lambda_p}$
        \item $\dim E_{\lambda_1} + \ldots + \dim E_{\lambda_p} = \dim E$
    \end{enumerate}
\end{theorem}
\begin{corollary}
   Si $f$ est un endomorphisme de  $E$ avec  $\dim E = n$ et  $f$ admet  $n$ valeurs propres  deux à deux distinctes, alors $f$ est diagonalisable.
\end{corollary}

Mais comme les valeurs propres sont les racines du polynôme caracteristique (voir prop \ref{prop:polynome-caracteristique}) on a:
\begin{prop}
    Soit $f$ un endomorphisme dans $E$ et  $\lambda$ une valeur propre d'ordre  $\alpha$ (i.e $\alpha$ est une racine de  $P_f(\lambda)$ d'ordre  $\alpha$, i.e  $P_f(\lambda) = (X - \lambda)^{\alpha}Q(X)$). Alors:
   \[
   \dim E_{\lambda} \le \alpha
   \] 
\end{prop}
\begin{theorem}
   Soit $f$ un endomorphisme dans  $E$ avec $\dim E = n$. Alors  $f$ est diagonalisable si et seulement si: 
   \begin{enumerate}
       \item $P_f(X)$ est \underline{scindé}, i.e:
            \[
                P_f(X) = (-1)^n (X - \lambda_1)^{\alpha_1} \cdot \ldots \cdot (X - \lambda_p)^{\alpha_p}
           \] 
           ($\lambda_i$ sont les racines donc les valeurs propres) et $\alpha_1 + \ldots + \alpha_p = n$. Alors, si la somme des multiplicités des racines est égale à la dimension de l'éspace vectoriel.
       \item Les dimensions des espaces propres sont \underline{maximales}, i.e $\forall i \in \{1, \ldots, p\}$
           \[
                \dim E_{\lambda_i} = \alpha_i 
           \] 
   \end{enumerate}
\end{theorem}
\begin{intuition}
   Ce n'est pas toujours facile de comprendre l'idée par les polynômes caracteristiques, alors une autre façon de voir ça est:
   \begin{enumerate}
       \item On trouve les valeurs propres: $\lambda_1, \ldots, \lambda_p$
       \item Puis on trouve les espaces propres: $E_{\lambda_i} = \ker(f - \lambda_i I)$
       \item On somme les dimension: $\dim E_{\lambda_1} + \ldots + \dim E_{\lambda_p} =: d$.  
           \begin{itemize}
               \item 
                   Si $d = \dim E$ i.e si la somme des dimension est égale à la dimension de l'espace  $E$, les espaces propres engendrent  $E$ et donc  $f$ est diagonalisable (car sa matrice peut s'écrire dans la base de ces vecteurs propres).
              \item Sinon le nombre de vecteurs propres libres ne suffit pas pour engendrer  $E$.
           \end{itemize}
   \end{enumerate}
\end{intuition}

\section{Les applications}
\subsection{Calcul de la puissance}\label{subsec:calcule-de-la-puissance-diagonalisation}
Alors, on est revenu là, où on a commencé, je vous rappele l'exercice du début du chapitre:
\begin{ex}
   Calculer 
   \[
   \begin{bmatrix} 
       3 & 1\\
       0 & 2
   \end{bmatrix}^{15} = \underbrace{
       \begin{bmatrix} 
       3 & 1\\
       0 & 2
       \end{bmatrix}
       \cdot
       \ldots
       \cdot
       \begin{bmatrix} 
       3 & 1\\
       0 & 2
       \end{bmatrix}
   }_{15 \text{ fois}}
   \] 
On rappelle, que les vecteurs propres de $A$ sont: 
\[
v_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix} \text{ et } v_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix} 
\] 
qui sont libres et engendrent $\R^2$ alors forment une base de $\R^2$, alors on peut écrire à dans cette nouvelle base et comme on a déjà trouvé:

\[
    A' = P^{-1}AP = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix} \begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix} 
\] 
dans la base $(v_1, v_2)$ avec la matrice de passage:
\[
    P = \begin{bmatrix} 1 & -1\\ 0 & 1 \end{bmatrix} \text{ et } P^{-1} = \begin{bmatrix} 1 & 1 \\ 0 & 1  \end{bmatrix}
\] 
De plus, en multipliant $A'$ avec $A'$, on a:  
\[
    A' \cdot A' = (P^{-1}AP)(P^{-1}AP) = P^{-1}A^2P = A'^2
\] 
d'où
\[
    A'^n = P^{-1}A^{n}P \implies PA'^nP^{-1} = PP^{-1}A^{n}PP^{-1} = A^{n}
\] 
Cela nous donne la possibilité de calculer d'abord la puissance de $A'$:
 \[
     A'^{15} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}^{15} = \begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}^{13} = \begin{bmatrix} 3^2 & 0 \\ 0 & 2^2 \end{bmatrix}\begin{bmatrix} 3 & 0 \\ 0 & 2 \end{bmatrix}^{13} = \begin{bmatrix} 3^{15} & 0 \\ 0 & 2^{15} \end{bmatrix}
\] 
Voilà, beaucoup plus facile, que calculer $A^15$ directement, alors il nous reste à revenir en base canonique:
 \[
     P \begin{bmatrix} 3^{15} & 0 \\ 0 & 2^{15} \end{bmatrix} P^{-1} = \begin{bmatrix} 1 & -1 \\ 0 & 1 \end{bmatrix} \begin{bmatrix} 3^{15} & 0 \\ 0 & 2^{15} \end{bmatrix}\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 3^{15} & 3^{15} - 2^{15} \\ 0 & 2^{15}\end{bmatrix} 
\] 
\end{ex}
Ce qui est très utile dans les matrices diagonales, c'est que la puissance de telle matrice égale à la même matrice avec les éléments diagonaux pris à la puissance, i.e:
\[
    A' = \begin{bmatrix} 
        \lambda_1 & 0 & \ldots & 0\\ 
        0 & \lambda_2 & \ldots & 0\\
        \vdots & \ddots & \ddots & \vdots\\
        0 & 0 & \ldots & \lambda_n
    \end{bmatrix} \implies A'^{n} = \begin{bmatrix} 
        \lambda_1 & 0 & \ldots & 0\\ 
        0 & \lambda_2 & \ldots & 0\\
        \vdots & \ddots & \ddots & \vdots\\
        0 & 0 & \ldots & \lambda_n
    \end{bmatrix}^{n} = \begin{bmatrix} 
        \lambda_1^n & 0 & \ldots & 0\\ 
        0 & \lambda_2^n & \ldots & 0\\
        \vdots & \ddots & \ddots & \vdots\\
        0 & 0 & \ldots & \lambda_n^n
    \end{bmatrix}
\] 
Généralisons: Si $A \in \mathcal{M}_n(K)$ est diagonalisable (i.e  il existe $P$ et  $A'$ telles que  $A' = P^{-1}AP$), alors:
\[
    A^{n} = P(A'^{n})P^{-1} = P\begin{bmatrix} 
        \lambda_1^n & 0 & \ldots & 0\\ 
        0 & \lambda_2^n & \ldots & 0\\
        \vdots & \ddots & \ddots & \vdots\\
        0 & 0 & \ldots & \lambda_n^n
    \end{bmatrix}P^{-1}
\] 

\subsection{Résolution d'un système de suites récurrentes}
Soit $(u_n)_{n \in \N}$ et $(v_n)_{n \in \N}$ deux suites telles que:
\begin{equation}\label{eq:systeme-appli-diagonalisation}
    \begin{cases}
        u_{n+1} = u_n - v_n\\
        v_{n+1} = 2u_n + 4v_n
    \end{cases}
\end{equation}
avec $u_0 = 2$ et $v_n = 1$. On pose $X_n = \begin{pmatrix} u_n \\ v_n \end{pmatrix} $, alors le système \ref{eq:systeme-appli-diagonalisation} s'écrit:
\[
    X_{n+1} = AX_n \quad \text{ avec } \quad A = \begin{pmatrix} 1 & -1\\ 2 & 4 \end{pmatrix} 
\] 
par récurrence on obtient:
\[
X_n = A^nX_0 \quad \text{ avec } X_0 = \begin{pmatrix} 2 \\ 1 \end{pmatrix} 
\] 

Alors, on est ramené au calcul de la puissance d'une matrice: $A^n$ ce qu'on a vu à la section ~\ref{subsec:calcule-de-la-puissance-diagonalisation}. Vous pouvez vérifiez qu'il existe $P \in GL_2(\R)$ tq 
\[
    P = \begin{pmatrix} -1 & 1 \\ 1 & -2 \end{pmatrix} \quad \text{ avec } \quad A = P\begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix}P^{-1}
\] 
et alors 
\[
    A^n = P\begin{pmatrix} 2^n & 0 \\ 0 & 3^n \end{pmatrix}P^{-1} = \begin{pmatrix} -1 & 1 \\ 1 & -2 \end{pmatrix}  \begin{pmatrix} 2^n & 0 \\ 0 & 3^n \end{pmatrix}  \begin{pmatrix} -2 & -1 \\ -1 & -1 \end{pmatrix}  =  
    \begin{pmatrix}
        2 \cdot 2^n - 3^n & 2^n - 3^n \\
        -2 \cdot 2^n + 2 \cdot 3^n & -2^n + 2 \cdot 3^n
    \end{pmatrix}
\] 
D'où
\[
\begin{pmatrix} u_n \\ v_n \end{pmatrix} = 
    \begin{pmatrix}
        2 \cdot 2^n - 3^n & 2^n - 3^n \\
        -2 \cdot 2^n + 2 \cdot 3^n & -2^n + 2 \cdot 3^n
    \end{pmatrix}
    \begin{pmatrix} 2 \\ 1 \end{pmatrix} 
    =
    \begin{pmatrix}
        4 \cdot 2^n - 2 \cdot 3^n + 2^n - 3^n \\
        -4 \cdot 2^n + 4 \cdot 3^n -2^n + 2 \cdot 3^n
    \end{pmatrix}
\] 
c'est-à-dire:
\[
\begin{cases}
   u_n = 5 \cdot 2^n - 3\cdot 3^n \\
   v_n = -5 \cdot 2^n + 6\cdot 3^n
\end{cases}
\] 

\subsection{Résolution des equations différentielles}
Soit à résoudre le système différentiel
\[
\left\{
\begin{aligned}
\frac{dx_1}{dt} &= a_{11}x_1 + \cdots + a_{1n}x_n \\
&\vdots \\
\frac{dx_n}{dt} &= a_{n1}x_1 + \cdots + a_{nn}x_n
\end{aligned}
\right.
\]
avec \( a_{ij} \in \mathbb{R} \) et \( x_i : \mathbb{R} \rightarrow \mathbb{R} \) dérivables.\\

Sous forme matricielle le système s’écrit :
\begin{equation}\label{eq:equation-differentielle-diagonalisation}
\frac{dX}{dt} = AX, \quad \text{où} \quad A = (a_{ij}), \quad X = 
\begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix}
\end{equation}

Supposons \( A \) diagonalisable. Il existe alors \( A' \) matrice diagonale et \( P \) matrice inversible telles que : 
\[
A' = P^{-1}AP.
\]
Si on considère \( A \) comme la matrice d’un endomorphisme dans la base canonique, \( A' \) est la matrice de \( f \) dans la base de vecteurs propres \( \{v_i\} \).\\
De même \( X \) est la matrice d’un vecteur \( \vec{x} \) dans la base canonique et \( X' = M(\vec{x})_{v_i} \), est liée à \( X \) par
\[
X' = P^{-1}X
\]
\begin{note}
   Attention! Dans cette section $X'$ ne décrit pas la dérivé, mais un vecteur noté  $X'$! 
\end{note}
En dérivant cette relation :
\[
\frac{dX'}{dt} = P^{-1} \frac{dX}{dt}
\]
(car \( A \) étant à coefficients constants, \( P \) sera aussi à coefficients constants). Donc :
\[
\frac{dX'}{dt} = P^{-1}AX = \left( P^{-1}AP \right) X' = A'X'
\]
Le système \ref{eq:equation-differentielle-diagonalisation} est donc équivalent au système
\[
\frac{dX'}{dt} = A'X'
\]

Ce système s’intègre facilement, car \( A' \) est diagonale.\\
Ainsi, on peut résoudre le système \( \frac{dX}{dt} = AX \) de la manière suivante :
\begin{enumerate}
    \item[a)] On diagonalise \( A \). Soit \( A' = P^{-1}AP \) une matrice diagonale semblable à \( A \);
    \item[b)] on intègre le système \( \frac{dX'}{dt} = A'X' \);
    \item[c)] on revient à \( X \) par \( X = PX' \).
\end{enumerate}

\subsection*{Exemple}

Soit le système
\[
\left\{
\begin{aligned}
\frac{dx}{dt} &= x - y \\
\frac{dy}{dt} &= 2x + 4y
\end{aligned}
\right.
\]

On a \( A' = \begin{pmatrix} 2 & 0 \\ 0 & 3 \end{pmatrix} \) et 
\( P = \begin{pmatrix} 1 & 1 \\ -1 & -2 \end{pmatrix} \)

Le système \( \frac{dX'}{dt} = A'X' \) s’écrit :
\[
\left\{
\begin{aligned}
\frac{dx'}{dt} &= 2x' \\
\frac{dy'}{dt} &= 3y'
\end{aligned}
\right.
\]
qui donne immédiatement 
\[
\left\{
\begin{aligned}
x' &= C_1 e^{2t} \\
y' &= C_2 e^{3t}
\end{aligned}
\right.
\]
et donc, en revenant à \( X \) par \( X = PX' \) :
\[
\begin{pmatrix}
x \\
y
\end{pmatrix}
=
\begin{pmatrix}
1 & 1 \\
-1 & -2
\end{pmatrix}
\begin{pmatrix}
C_1 e^{2t} \\
C_2 e^{3t}
\end{pmatrix}
=
\begin{pmatrix}
C_1 e^{2t} + C_2 e^{3t} \\
- C_1 e^{2t} - 2C_2 e^{3t}
\end{pmatrix}
\]

c’est-à-dire :
\[
\left\{
\begin{aligned}
x &= C_1 e^{2t} + C_2 e^{3t} \\
y &= -C_1 e^{2t} - 2C_2 e^{3t}
\end{aligned}
\right.
\]

\section{Trigonalisation}
Une matrice $A \in \mathcal{M}_n(K)$ est dite triangulaire supérieure si elle est de la forme:
\[
         A = 
         \begin{bmatrix} 
         a_{1,1} & a_{1,2} & \ldots & a_{1,n} \\
         0 & a_{2,2} & \ddots & \vdots\\
         \vdots & \ddots & \ddots & a_{n-1,n}\\
         0 & \ldots & 0 & a_{n,n}
        \end{bmatrix}
\] 
respectivement triangulaire inférieure:
\[
         A = 
         \begin{bmatrix} 
         a_{1,1} & 0 & \ldots & 0 \\
         a_{2, 1} & a_{2,2} & \ddots & \vdots\\
         \vdots & \ddots & \ddots & 0\\
         a_{n, 1} & \ldots & a_{n, n-1} & a_{n,n}
        \end{bmatrix} 
\] 
\begin{remark}
   Toute matrice $A$ triangulaire supérieure est semblable à une matrice triangulaire inférieure. 
\end{remark}
\begin{proof}
    Soit $A$ une matrice triangulaire supérieure et  $f$ l'endomorphisme de  $K^n$ qui dans la base $\{e_1, \ldots, e_n\}$ est représentée par la matrice $A$, alors:
     \[
    \begin{cases}
        f(e_1) = a_{1, 1} e_1\\
        f(e_2) = a_{1, 2} e_1 + a_{2, 2}e_2\\
        \vdots \\
        f(e_n) = a_{1, n} e_1 + a_{2, n}e_2 + \ldots + a_{n, n}e_n
    \end{cases} 
    \iff
         A = 
         \begin{bmatrix} 
         a_{1,1} & a_{1,2} & \ldots & a_{1,n} \\
         0 & a_{2,2} & \ddots & \vdots\\
         \vdots & \ddots & \ddots & a_{n-1,n}\\
         0 & \ldots & 0 & a_{n,n}
        \end{bmatrix}
    \] 
    Considérons la base 
    \[
    \varepsilon_1 = e_n, \quad \varepsilon_2 = e_{n-1}, \quad \ldots, \quad \varepsilon_n = e_1
    \] 
    alors, on a:
    \[
    \begin{cases}
        f(\underbrace{\varepsilon_1}_{e_n}) = = a_{1, n}\underbrace{\varepsilon_n}_{e_1} + a_{2, n}\underbrace{\varepsilon_{n-1}}_{e_2} + \ldots + a_{n, n}\underbrace{\varepsilon_1}_{e_n} \\
        f(\underbrace{\varepsilon_2}_{e_{n-1}}) = = a_{1, n-1}\underbrace{\varepsilon_n}_{e_1} + \ldots + a_{n-1, n-1}\underbrace{\varepsilon_2}_{e_{n-1}} \\
        \vdots \\
        f(\underbrace{\varepsilon_n}_{e_1}) = a_{1,1}\underbrace{\varepsilon_n}_{e_1}
    \end{cases}
    \] 
    donc 
    \[
    A' = M(f)_{\varepsilon_{i}} = 
    \begin{bmatrix} 
        a_{n, n}   &              & \ldots & 0\\
        a_{n-1, n} & a_{n-1, n-1} & \ldots & 0\\
        \vdots     &              & \ddots & \\
        a_{1, n}   & \ldots       &        & a_{1,1}
    \end{bmatrix} 
    \] 
\end{proof}
\subsection{L'intuition géométrique de la diagonalisation}
Rappellons la diagonalisation. La matrice $A$ représentative de l'endomorphisme  $f$ dans $K^n = \operatorname{Vect}(e_1, \ldots, e_n)$ ést diagonalisable, s'il existe suffisament de sous-espaces vectoriels $\{F_1, \ldots, F_n\}$ de dimension $1$ chaqun, tel que  $K^n = F_1 \oplus \ldots \oplus F_n$ et $\forall i \in \{1, \ldots, n\}, f(F_i) \subset F_i$ (un vecteur après l'application de $f$ reste dans l'espace). Ce qu'on peut voir géométriquement:
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
        axis lines=middle, 
        grid=major, 
        xlabel={$x$}, 
        ylabel={$y$}, 
        title={Eigenvector Transformation},
        ymin=-2, ymax=5,
        xmin=-3, xmax=5
    ]
        \addplot[thick, blue] coordinates {(-3, 0) (5, 0)};
        \node[below] at (axis cs:4.8,0) {$F_1$};

        \addplot[thick, blue] coordinates {(-3,3) (5, -5)};
        \node[below, left] at (axis cs:-2.2,2.2) {$F_2$};

        % Define original vectors
        \addplot[->, thick, red] coordinates {(0,0) (1,0)};
        \node[right] at (axis cs:1,0) {$v_1$};

        \addplot[->, thick, red] coordinates {(0,0) (-1, 1)};
        \node[above] at (axis cs:-1,1) {$v_2$};

        % \addplot[->, thick, blue] coordinates {(0,0) (1,1)};
        % \node[right] at (axis cs:1,1) {$v_3$};
        %
        % \addplot[->, thick, blue] coordinates {(0,0) (-1,2)};
        % \node[left] at (axis cs:-1,2) {$v_4$};

        % Define transformed vectors
        \addplot[->, thick, red, dashed] coordinates {(0,0) (3,0)};
        \node[right] at (axis cs:3,0) {$A v_1$};

        \addplot[->, thick, red, dashed] coordinates {(0,0) (-2,2)};
        \node[above] at (axis cs:-2,2) {$A v_2$};

        %
    \end{axis}
\end{tikzpicture}
\end{center}
On sait déjà que tel endomorphisme est très utile mais ça n'arrive pas souvent qu'on peut le diagonaliser, alors, ça serait utile d'avoir quelque chose plus général mais encore semblable à la diagonalisation.
\subsection{L'intuition géométrique de la trigonalisation}
La géométrie de l'endomorphisme trigonalisable est similaire mais quand même différente. Soit $A$ une matrice représentative de l'endomorphisme  $f$ dans  $K^n$. Il est trigonalisable s'il existe une base  $\{v_1, \ldots, v_n\}$ de $K^n$, notons  $F_1 = \operatorname{Vect}(v_1), F_2 = \operatorname{Vect}(v_1, v_2), \ldots, F_n = \operatorname{Vect}(v_1, v_2, \ldots, v_n)$ tels que
\[
F_1 \subset F_2 \subset \ldots \subset F_n
\] 
et 
\[
    \forall i \in \{1, \ldots, n\}, f(F_i) \subset F_i
\] 
Voyez-vous la similarité? L'endomorphisme est stable par le sous-espace! Le vecteur appliqué à $f$ ne quite jamais son sous-espace. Prennons pour l'exemple la matrice suivante:
\[
    A = \begin{bmatrix} 
        1 & 1 & 0\\
        0 & 2 & 1\\
        0 & 0 & 3
    \end{bmatrix} = \operatorname{Mat}(f)_{e_i}
\] 
% \[
% \begin{cases}
%     v_1 := \begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix} \in F_1, f(\begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix}) = \begin{pmatrix} 2 \\ 0 \\ 0 \end{pmatrix} \in F_1\\
%     v_2 = 
% \end{cases}
% \] 

\begin{center}
    
\begin{tikzpicture}
  \begin{axis}[
      view={60}{30},
      axis lines=center,
      xlabel={$x$},
      ylabel={$y$},
      zlabel={$z$},
      xmin=-1, xmax=4,
      ymin=-1, ymax=4,
      zmin=-1, zmax=4,
      samples=10,
      domain=-1:4,
      y domain=-1:4,
      z buffer=sort,
      width=10cm, height=8cm,
  ]
    % F1: Invariant line (x-axis)
    \addplot3[blue, ultra thick, domain=-1:3]({x},{0},{0});
    \node[blue] at (axis cs:3.2,0,0) {$F_1$};

    % \addplot3[->, black, thick] coordinates {(0, 0, 0) (2, 0, 0)};
    % \node[black, above] at (axis cs:2,0,0) {$v_1 = f(v_1)$};

    % F2: Invariant plane (xy-plane)
    \addplot3[
      surf,
      fill=red,
      opacity=0.3,
      shader=interp,
      samples=2,
      domain=-1:3,
      y domain=-1:3,
    ]
    ({x},{y},{0});
    \node[red] at (axis cs:1.5,1.5,0.2) {$F_2$};

    % Optionally: Draw a vector in F3 (illustrating the full space)
    \addplot3[black,->,thick] coordinates {(0,0,0) (2,1,2)};
    \node at (axis cs:2.1,1.1,2.3) {$v\in F_3$};

  \end{axis}
\end{tikzpicture}
\end{center}
Comme on a de l'intuition de l'endomorphisme trigonalisable, revenons sur les maths pures.
\subsection{Théorie}
\begin{theorem}\label{thm:trigonalisable-si-scinde}
    Un endomorphisme est trigonalisable dans $K$ si et seulement si son polyôme caractéristique est scindé dans  $K$. 
    \par 
    Ça veut dire que le polynôme caractéristique admet exactement $n$ racine où  $n = \dim(E)$ et dont s'écrit:
     \[
         P_f(X) = (-1)^n(X - \lambda_1)^{\alpha_1}\cdots(X - \lambda_p)^{\alpha_p}
    \] 
    avec $\alpha_1 + \ldots + \alpha_p = n$
\end{theorem}
\begin{preuve} -
   \begin{itemize}
       \item[($\implies$)] Supposons l'endomorphisme $f$ est trigonalisable et soit une base  $\{e_1, \ldots, e_n\}$ telle que
           \[
               M(f)_{e_i} = \begin{pmatrix} 
                   a_{1,1} &            & & *\\
                   0       & a_{2, 2}   & & \\
                   \vdots  &      & \ddots & \\
                   0        & \ldots    & 0 & a_{n, n}
               \end{pmatrix} 
           \] 
           On a:
           \[
            P_f(X) = \det \begin{pmatrix} 
                   a_{1,1} - X &            & & *\\
                   0       & a_{2, 2} - X   & & \\
                   \vdots  &      & \ddots & \\
                   0        & \ldots    & 0 & a_{n, n} - X
               \end{pmatrix} = (a_{1,1} - X) \cdots (a_{n,n} - X)
           \] 
           Donc $P_f(X)$ est bien scindé (on peut remarquer que ses racines sont les valeurs propres de $f$).
       \item[($\impliedby$)] Supposons $P_f(X)$ scindé et montrons par récurrence que  $f$ trigonalisable.
            \par
            Pour  $n=1$ triviale.
            \par
            Supposons que le résultat est vrai à l'ordre  $n-1$. Or  $P_f(X)$ est scindé il admet au moins une racine  $\lambda_1 \in K$ et donc un vecteur propre $\varepsilon_1 \in E_{\lambda_1}$. Complétons $\{\varepsilon_1\}$ en une base $\{\varepsilon_1, \ldots, \varepsilon_n\}$, donc on a:
            \[
            A = M(f)_{\varepsilon_i} = \begin{pmatrix} 
                \lambda_1 & b_2 & \ldots & b_n\\
                0         &     &        &    \\
                \vdots    &     & B      &    \\
                0         &     &        &
            \end{pmatrix}, \quad \text{où:} B \in \mathcal{M}_{n-1}(K)
            \] 
            Soit $F = \operatorname{Vect}(\varepsilon_2, \ldots, \varepsilon_n)$ et $g: F \to F$ l'unique endomorphisme de $F$ tel que  $M(g)_{\varepsilon_2, \ldots, \varepsilon_n} = B$, on a:
            \[
            P_f(X) = \det(A - XI_n) = (\lambda_1 - X)\det(B - XI_{n-1}) = (\lambda_1 - X)P_g(X)
            \] 
            Or $P_f(X)$ est scindé,  $P_g(X)$ l'est aussi et d'après l'hypothèse de récurrence  $B$ est trigonalisable, donc il existe une base  $\{v_2, \ldots, v_n\}$ dans laquelle $M(g)_{v_2, \ldots, v_n}$ est triangulaire et donc la matrice de $f$ dans la base  $\{\varepsilon_1, v_2, \ldots, v_n\}$ est triangulaire donc $f$ est trigonalisable.
   \end{itemize} 
\end{preuve}
\begin{corollary}
    Toute matrice $A \in \mathcal{M}_n(\mathbb{C})$  est semblable à une matrice triangulaire de $\mathcal{M}_n(\mathbb{C})$.
\end{corollary}
\begin{intuition}
    D'après le cours d'algèbre abstrait, tout polyôme dans $\mathbb{C}$ est scindé. 
\end{intuition}
\begin{remark}-
   \begin{enumerate}
       \item Si $A$ est trigonalisable et  $A'$ triangulaire semblable à $A$, donc  $A'$ a des valeurs prorpes sur les diagonales.
       \item Toute matrice $A \in \mathcal{M}_n(K)$ est trigonalisable sur la clôture $K'$ de  $K$. (e.g: $A \in \mathcal{M}_n(\R)$ est trigonalisable sur $\mathbb{C}$).
   \end{enumerate} 
\end{remark}
\begin{corollary}
    Soit $A \in \mathcal{M}_n(K)$ est $\{\lambda_1, \ldots, \lambda_n\}$ ses valeurs propres, donc
    \begin{align*}
        \operatorname{Tr}(A) = \lambda_1 + \ldots + \lambda_n\\
        \det(A) = \lambda_1 \cdot \ldots \cdot  \lambda_n
    \end{align*}
\end{corollary}
\begin{proof}
    On $A' \in \mathcal{M}_n(K')$ triangulaire semblable à $A$ (reappel: clôture $K'$ sur  $K$), donc les valeurs prorpes sont sur les diagonales de $A'$. Or les matrices semblables ont les même traces et détérminant, donc $\operatorname{Tr}(A) = \operatorname{Tr}(A') = \lambda_1 + \ldots + \lambda_n$ et $\det(A) = \det(A') = \lambda_1 \cdot \ldots \cdot \lambda_n$.
\end{proof}

On va montrer le processus de la trigonalisation avec l'exemple suivant:

\begin{eg}
   Soit la matrice 
   \[
   A = \begin{pmatrix}
       -4 & 0 & -2 \\
       0 & 1 & 0\\
       5 & 1 & 3
   \end{pmatrix}
   \] 
   On a le polynôme caractéristique $P_A(X) = -(X - 1)^2(X + 2)$ qui est scindé dans $\R$, donc $A$ est trigonalisable (d'après le théorème \ref{thm:trigonalisable-si-scinde}), alors si on regarde $A$ comme un endomorphisme dans la base canonique, on sait qu'il existe une base  $\{v_i\}$ de  $\R^3$ telle que:
   \[
       M(f)_{v_i} = \begin{pmatrix} 
           1 & a & b\\
           0 & 1 & c\\
           0 & 0 & -2
       \end{pmatrix} 
   \] 
   je rappelle que cela signifie:
   \begin{align}\label{eq:trigon-example-system}
       \begin{cases}
           f(v_1) = v_1\\
           f(v_2) = a v_1 + v_2\\
           f(v_3) = b v_1 + c v_2 - 2 v_3
       \end{cases}
    \end{align}

   Commençons par la recherche de $v_1$. On sait que  $v_1$ est un vecteur propre correspondant à la valeur propre  $\lambda_1 = 1$, i.e  $(f - \operatorname{Id})v_1 = 0$, donc calculons $(A - I)v_1 = 0$ (autrement dire, on cherche $v_1$ qui engendre  $\ker(A - I)$) :
   \begin{align*}
       (A - I) \begin{pmatrix} x \\ y \\ z \end{pmatrix} \iff 
       \begin{cases}
           -5x    -2z & = 0\\
            5x  +y +2z &=0
       \end{cases}
    \end{align*}
    Alors, on peut prendre $v_1 = \begin{pmatrix} 2 \\ 0 \\ -5 \end{pmatrix} $ (autrement dire $\ker(A - I) = \operatorname{Vect}(\begin{pmatrix} 2 \\ 0 \\ -5 \end{pmatrix})$).

    Ensuite, cherchons $v_2$, d'après \ref{eq:trigon-example-system}, 
    \begin{align*}
        &f(v_2) = av_1 + v_2 \\
        \implies &f(v_2) - v_2 = av_1\\
        \implies &(f - I)v_2 = a v_1\\
        \implies &(A - I)v_2 = a v_1
    \end{align*}
    Donc on a:
    \[
        (A - I)\begin{pmatrix} x \\ y \\ z \end{pmatrix} = a \begin{pmatrix} 2 \\ 0 \\ -5 \end{pmatrix} \iff
        \begin{cases}
            -5x - 2z = 2a\\
            5x + y + 2z = -5a
        \end{cases}
    \] 
    Alors, en prennant $a = 1$, on a 
    \[
        \begin{cases}
            -5x - 2z = 2\\
            5x + y + 2z = -5
        \end{cases}
    \] 
    donc $v_2 = \begin{pmatrix} -2 \\ -3 \\ 4 \end{pmatrix} $ (juste résolution du système).

    Pour le $v_3$ on a deux choix:
     \begin{enumerate}
        \item soit procéder pareil avec la résolution du système
        \item soit remarquer qu'il existe un vecteur propre de $A$ correspondant à la valeur propre  $-2$, i.e  $\exists v_3 \in \R^3$ tq $f(v_3) = -2 v_3$, alors on peut prendre ce vecteur  $v_3$ et donc mettre  $b = c = 0$.
    \end{enumerate}
    \begin{remark}
       Pourquoi on peut faire comme ça? Parce que pour toute valeur propre de $f$ il existe toujours un espace propre de multiplicité au moins  $1$, donc pour  la valeur propre  $-2$ aussi. 
    \end{remark}
    Alors cherchons le $v_3$:
     \[
         (A + 2I)v_3 = 0 \iff \begin{cases}
             -2x - 2z = 0\\
             3y = 0
         \end{cases}
    \]
    donc on peut prendre $v_3 = \begin{pmatrix} 1 \\ 0 \\ -1 \end{pmatrix} $.
    
    Par conséquent, la matrice $A$ est semblable à 
     \[
    A' = M(f)_{v_i} = \begin{pmatrix} 
        1 & 1 & 0\\
        0 & 1 & 0\\
        0 & 0 & -2
    \end{pmatrix} 
    \] 
    avec la matrice de passage:
    \[
    P = \|v_1, v_2, v_3\| = \begin{pmatrix} 
        2 & -2 & 1\\
        0 & -3 & 0\\
        -5 & 4 & -1
    \end{pmatrix} 
    \] 
\end{eg}
